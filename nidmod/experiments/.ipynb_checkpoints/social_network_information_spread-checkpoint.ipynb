{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ddce6e9",
   "metadata": {},
   "source": [
    "## Online Social Network Information Spread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac554600",
   "metadata": {},
   "source": [
    "This experiment tests how different data integration setups can affect the spread of information in online social networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f434c1",
   "metadata": {},
   "source": [
    "We use the feature setup, network integrator, and custom diffusion model and multinetwork diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b856dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no display found. Using non-interactive Agg backend\n",
      "no display found. Using non-interactive Agg backend\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:/Users/jnevin/Documents/GitHub/nidmod\")\n",
    "\n",
    "from nidmod.datahandling.dataintegration import FeatureSetup, MatchClassifier, NetworkIntegrator\n",
    "from nidmod.diffusionmodel.diffusionmodel import CustomDiffusionModel\n",
    "from nidmod.parameter_sweeper import MultiNetworkDiffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e662c8e",
   "metadata": {},
   "source": [
    "We have nodes and edges for the myspace and last.fm graphs, plus matches between the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6378a15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "myspace_nodes = pd.read_csv('myspace.nodes', sep = '\\t', header = None)\n",
    "myspace_edges = pd.read_csv('myspace.edges', sep = ' ', header = None)\n",
    "\n",
    "lastfm_nodes = pd.read_csv('lastfm.nodes', sep = '\\t', header = None)\n",
    "lastfm_edges = pd.read_csv('lastfm.edges', sep = ' ', header = None)\n",
    "\n",
    "true_mapping = pd.read_csv('lastfm-myspace.map.raw', sep = ' ', header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf2ff11",
   "metadata": {},
   "source": [
    "We relabel the nodes and edges to be clear to which network they belong and construct the graphs from the edge list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "290d3fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "myspace_nodes[0] = myspace_nodes[0].map(lambda x: 'myspace_' + str(x))\n",
    "myspace_nodes = myspace_nodes.rename(columns={0: 'myspace_index', 1: 'username'})\n",
    "myspace_nodes = myspace_nodes.set_index('myspace_index')\n",
    "\n",
    "lastfm_nodes[0] = lastfm_nodes[0].map(lambda x: 'lastfm_' + str(x))\n",
    "lastfm_nodes = lastfm_nodes.rename(columns={0: 'lastfm_index', 1: 'username'})\n",
    "lastfm_nodes = lastfm_nodes.set_index('lastfm_index')\n",
    "\n",
    "myspace_edges[0] = myspace_edges[0].map(lambda x: 'myspace_' + str(x))\n",
    "myspace_edges[1] = myspace_edges[1].map(lambda x: 'myspace_' + str(x))\n",
    "\n",
    "lastfm_edges[0] = lastfm_edges[0].map(lambda x: 'lastfm_' + str(x))\n",
    "lastfm_edges[1] = lastfm_edges[1].map(lambda x: 'lastfm_' + str(x))\n",
    "\n",
    "myspace_graph = nx.Graph()\n",
    "for edge in myspace_edges.values:\n",
    "    myspace_graph.add_edge(edge[0], edge[1])\n",
    "    \n",
    "lastfm_graph = nx.Graph()\n",
    "for edge in lastfm_edges.values:\n",
    "    lastfm_graph.add_edge(edge[0], edge[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4742be",
   "metadata": {},
   "source": [
    "We work with only the largest connected component (almost the entire graph) plus only consider duplicates for seed nodes if they are in both connected components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93362fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_cc = max(nx.connected_components(myspace_graph), key=len)\n",
    "myspace_graph = myspace_graph.subgraph(largest_cc)\n",
    "\n",
    "largest_cc = max(nx.connected_components(lastfm_graph), key=len)\n",
    "lastfm_graph = lastfm_graph.subgraph(largest_cc)\n",
    "\n",
    "myspace_nodes = myspace_nodes.loc[list(myspace_graph.nodes())]\n",
    "lastfm_nodes = lastfm_nodes.loc[list(lastfm_graph.nodes())]\n",
    "\n",
    "useable_mappings = true_mapping.loc[(true_mapping[1].isin(myspace_nodes.username)) & (true_mapping[0].isin(lastfm_nodes.username))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c2462a",
   "metadata": {},
   "source": [
    "We define neighbour exploration to create the subsampled graphs from the seed nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb8a7de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbour_explore(graph, seed_nodes, steps, size):\n",
    "    start_nodes = seed_nodes\n",
    "    included_nodes = set(start_nodes)\n",
    "\n",
    "    already_checked_nodes = set()\n",
    "    to_check_nodes = start_nodes\n",
    "\n",
    "    for i in range(steps):\n",
    "        nodes_to_add = set()\n",
    "        for node in to_check_nodes:\n",
    "            already_checked_nodes = already_checked_nodes | set([node])\n",
    "            neighbours = [n for n in graph.neighbors(node)]\n",
    "            nodes_to_add = nodes_to_add | set(np.random.choice(neighbours, replace = False, size = min(size, len(neighbours))))\n",
    "        included_nodes = included_nodes | set(nodes_to_add)\n",
    "        to_check_nodes = nodes_to_add - already_checked_nodes\n",
    "        \n",
    "    return included_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971d559f",
   "metadata": {},
   "source": [
    "We define the diffusion model and simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1368ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "statuses = ['Susceptible', 'Infected', 'Removed']\n",
    "compartments = {'NodeStochastic': {'c1': [0.02, 'Infected'], 'c2': [0.01]}}\n",
    "transition_rules = [[\"Susceptible\", \"Infected\", \"c1\"], [\"Infected\", \"Removed\", \"c2\"]]\n",
    "model_parameters = [['fraction_infected', 0.1]]\n",
    "simulation_parameters = [25, 600, None, 5]\n",
    "model_name = 'sir'\n",
    "\n",
    "custom_diffusion_model = CustomDiffusionModel(statuses, compartments,\n",
    "                                             transition_rules, model_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e78501",
   "metadata": {},
   "source": [
    "We test different string thresholds with a Levenshtein similarity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ebfdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "levenshtein_graph_df = pd.DataFrame()\n",
    "levenshtein_diff_df = pd.DataFrame()\n",
    "\n",
    "# we test 25 different initial seed mappings\n",
    "for seed_node_num in range(25):\n",
    "    # we choose 200 random seed mappings\n",
    "\n",
    "    seed_mappings = useable_mappings.loc[np.random.choice(list(useable_mappings.index), size = 200, replace = False)]\n",
    "\n",
    "    # and then find the subsampled networks based on these seed nodes\n",
    "\n",
    "    myspace_seed_nodes = myspace_nodes.loc[myspace_nodes.username.isin(seed_mappings[1])].index\n",
    "    lastfm_seed_nodes = lastfm_nodes.loc[lastfm_nodes.username.isin(seed_mappings[0])].index\n",
    "\n",
    "    myspace_neighbour_nodes = neighbour_explore(myspace_graph, myspace_seed_nodes, 2, 5)\n",
    "    lastfm_neighbour_nodes = neighbour_explore(lastfm_graph, lastfm_seed_nodes, 2, 5)\n",
    "\n",
    "    myspace_neighbour_graph = myspace_graph.subgraph(myspace_neighbour_nodes)\n",
    "    lastfm_neighbour_graph = lastfm_graph.subgraph(lastfm_neighbour_nodes)\n",
    "\n",
    "    myspace_nodes_reduced = myspace_nodes.loc[list(myspace_neighbour_graph.nodes())]\n",
    "    lastfm_nodes_reduced = lastfm_nodes.loc[list(lastfm_neighbour_graph.nodes())]\n",
    "\n",
    "    index_setup = {'Full': [[]]}\n",
    "    integrated_graphs = []\n",
    "\n",
    "    # we test a number of differet string thresholds\n",
    "    tested_thresholds = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9]\n",
    "\n",
    "    for string_threshold in tested_thresholds:\n",
    "        compare_setup = {'String': [['username', 'username', 'levenshtein', string_threshold]]}\n",
    "\n",
    "        feature_setup = FeatureSetup(index_setup, compare_setup, myspace_nodes_reduced, lastfm_nodes_reduced)\n",
    "        features = feature_setup.calculate_features()\n",
    "\n",
    "        pred_matches = features.loc[features[0] > 0]\n",
    "        pred_matches = pred_matches.index\n",
    "\n",
    "        network_integrator = NetworkIntegrator([myspace_neighbour_graph, lastfm_neighbour_graph], pred_matches)\n",
    "        integrated_graphs.append(network_integrator.integrate_network('multigraph_walktrap_integration'))\n",
    "\n",
    "    multi_network_diffusion = MultiNetworkDiffusion(integrated_graphs, custom_diffusion_model)\n",
    "    graph_assc_results_analysers = multi_network_diffusion.run_diffusion_model(simulation_parameters)\n",
    "    final_stats = graph_assc_results_analysers.get_average_stat_comparison()\n",
    "\n",
    "    # we track the size of the graphs and the proportion of the population adopting the information\n",
    "    levenshtein_graph_df = levenshtein_graph_df.append(dict(zip(tested_thresholds, \n",
    "                                                               [len(integrated_graphs[i].nodes()) for i in range(len(integrated_graphs))])), ignore_index = True)\n",
    "\n",
    "    levenshtein_diff_df = levenshtein_diff_df.append(dict(zip(tested_thresholds, \n",
    "                                                              final_stats['Removed_final'].values)), ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdac1e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "levenshtein_graph_df = pd.DataFrame()\n",
    "levenshtein_diff_df = pd.DataFrame()\n",
    "\n",
    "# we test 25 different initial seed mappings\n",
    "for seed_node_num in range(1):\n",
    "    # we choose 200 random seed mappings\n",
    "\n",
    "    seed_mappings = useable_mappings.loc[np.random.choice(list(useable_mappings.index), size = 200, replace = False)]\n",
    "\n",
    "    # and then find the subsampled networks based on these seed nodes\n",
    "\n",
    "    myspace_seed_nodes = myspace_nodes.loc[myspace_nodes.username.isin(seed_mappings[1])].index\n",
    "    lastfm_seed_nodes = lastfm_nodes.loc[lastfm_nodes.username.isin(seed_mappings[0])].index\n",
    "\n",
    "    myspace_neighbour_nodes = neighbour_explore(myspace_graph, myspace_seed_nodes, 2, 5)\n",
    "    lastfm_neighbour_nodes = neighbour_explore(lastfm_graph, lastfm_seed_nodes, 2, 5)\n",
    "\n",
    "    myspace_neighbour_graph = myspace_graph.subgraph(myspace_neighbour_nodes)\n",
    "    lastfm_neighbour_graph = lastfm_graph.subgraph(lastfm_neighbour_nodes)\n",
    "\n",
    "    myspace_nodes_reduced = myspace_nodes.loc[list(myspace_neighbour_graph.nodes())]\n",
    "    lastfm_nodes_reduced = lastfm_nodes.loc[list(lastfm_neighbour_graph.nodes())]\n",
    "\n",
    "    index_setup = {'Full': [[]]}\n",
    "    integrated_graphs = []\n",
    "\n",
    "    # we test a number of differet string thresholds\n",
    "    tested_thresholds = [0.6, 0.65]\n",
    "\n",
    "    for string_threshold in tested_thresholds:\n",
    "        compare_setup = {'String': [['username', 'username', 'levenshtein', string_threshold]]}\n",
    "\n",
    "        feature_setup = FeatureSetup(index_setup, compare_setup, myspace_nodes_reduced, lastfm_nodes_reduced)\n",
    "        features = feature_setup.calculate_features()\n",
    "\n",
    "        pred_matches = features.loc[features[0] > 0]\n",
    "        pred_matches = pred_matches.index\n",
    "\n",
    "        network_integrator = NetworkIntegrator([myspace_neighbour_graph, lastfm_neighbour_graph], pred_matches)\n",
    "        integrated_graphs.append(network_integrator.integrate_network('multigraph_walktrap_integration'))\n",
    "\n",
    "    multi_network_diffusion = MultiNetworkDiffusion(integrated_graphs, custom_diffusion_model)\n",
    "    graph_assc_results_analysers = multi_network_diffusion.run_diffusion_model(simulation_parameters)\n",
    "    final_stats = graph_assc_results_analysers.get_average_stat_comparison()\n",
    "\n",
    "    # we track the size of the graphs and the proportion of the population adopting the information\n",
    "    levenshtein_graph_df = levenshtein_graph_df.append(dict(zip(tested_thresholds, \n",
    "                                                               [len(integrated_graphs[i].nodes()) for i in range(len(integrated_graphs))])), ignore_index = True)\n",
    "\n",
    "    levenshtein_diff_df = levenshtein_diff_df.append(dict(zip(tested_thresholds, \n",
    "                                                              final_stats['Removed_final'].values)), ignore_index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
